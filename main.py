# rag_system_async.py - –ü–û–õ–ù–ê–Ø –í–ï–†–°–ò–Ø –° COLBERT –ò –°–û–•–†–ê–ù–ï–ù–ò–ï–ú –ì–†–ê–§–ê –õ–ï–ô–î–ï–ù–ê

# Python ‚â•3.10

from __future__ import annotations

import asyncio, hashlib, json, logging, os, time, re

from dataclasses import dataclass

from functools import lru_cache

from typing import Any, List, Protocol, Optional

from pathlib import Path

import faiss  # pip install faiss-cpu

import numpy as np

import torch

from cachetools import TTLCache  # pip install cachetools

from langchain_community.document_loaders import (
    UnstructuredPDFLoader,
    UnstructuredWordDocumentLoader,
    TextLoader
)

from langchain_community.vectorstores import FAISS

from langchain_community.retrievers import BM25Retriever

from langchain_huggingface import HuggingFaceEmbeddings

from langchain_ollama import OllamaLLM

from langchain_experimental.text_splitter import SemanticChunker

from langchain.text_splitter import RecursiveCharacterTextSplitter

from neo4j import AsyncGraphDatabase

# ‚úÖ –ù–û–í–´–ï –ò–ú–ü–û–†–¢–´ –î–õ–Ø COLBERT
try:
    from ragatouille import RAGPretrainedModel

    COLBERT_AVAILABLE = True
    print("‚úÖ ColBERT (RAGatouille) –¥–æ—Å—Ç—É–ø–µ–Ω")
except ImportError:
    COLBERT_AVAILABLE = False
    print("‚ùå ColBERT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ragatouille")


# -----------------------------------------------------------------------------
# 1. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏ –±–∞–∑–æ–≤—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã
# -----------------------------------------------------------------------------

@dataclass
class RAGConfig:
    folder_path: str
    faiss_path: str
    neo4j_uri: str
    neo4j_user: str
    neo4j_password: str
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    batch_size: int = 8
    ttl_cache_sec: int = 3_600
    faiss_ivf_thr: int = 1_000
    faiss_nprobe: int = 32
    log_level: int = logging.INFO
    chunk_size: int = 1000
    chunk_overlap: int = 200
    use_semantic_chunking: bool = True
    # ‚úÖ –ù–û–í–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –°–û–•–†–ê–ù–ï–ù–ò–Ø/–ó–ê–ì–†–£–ó–ö–ò –ì–†–ê–§–ê
    force_rebuild_graph: bool = True  # –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ True, –ø–æ—Ç–æ–º False
    save_graph_structure: bool = True  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –≥—Ä–∞—Ñ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è
    graph_metadata_dir: str = "graph_data/"  # –ü–∞–ø–∫–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    auto_load_existing: bool = True  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –≥—Ä–∞—Ñ
    # ‚úÖ –ù–û–í–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø COLBERT
    use_colbert_reranker: bool = True
    colbert_model: str = "colbert-ir/colbertv2.0"
    # ‚úÖ –£–õ–£–ß–®–ï–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò LLM –î–õ–Ø –ü–†–ï–î–û–¢–í–†–ê–©–ï–ù–ò–Ø –ü–û–í–¢–û–†–ï–ù–ò–ô
    llm_temperature: float = 0.4  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
    llm_repeat_penalty: float = 1.05  # –ú—è–≥–∫–æ–µ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
    llm_repeat_last_n: int = 128  # –û–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
    llm_top_p: float = 0.85  # –ë–æ–ª–µ–µ –∏–∑–±–∏—Ä–∞—Ç–µ–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
    llm_top_k: int = 40  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    llm_num_predict: int = 2048  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
    llm_stop_sequences: List[str] = None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –≤ __post_init__

    def __post_init__(self):
        if self.llm_stop_sequences is None:
            # ‚úÖ –°–¢–û–ü-–ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ò –î–õ–Ø –ü–†–ï–î–û–¢–í–†–ê–©–ï–ù–ò–Ø –ü–û–í–¢–û–†–ï–ù–ò–ô
            self.llm_stop_sequences = [
                "\n\n\n",  # –ú–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
                "–ü–æ–≤—Ç–æ—Ä—è—é",  # –†—É—Å—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                "–ï—â–µ —Ä–∞–∑",
                "–°–Ω–æ–≤–∞",
                "–û–ø—è—Ç—å",
                "–ö–∞–∫ —è —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª",
                "–ö–∞–∫ —É–∂–µ –±—ã–ª–æ —Å–∫–∞–∑–∞–Ω–æ",
                "Repeating",  # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è
                "Again,",
                "As I mentioned",
                "As stated before",
            ]


class Retriever(Protocol):
    async def retrieve(self, query: str, k: int) -> List[Any]: ...


# -----------------------------------------------------------------------------
# 2. –õ–æ–≥–≥–µ—Ä –∏ —É—Ç–∏–ª–∏—Ç—ã
# -----------------------------------------------------------------------------

def init_logger(level: int) -> logging.Logger:
    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[
            logging.FileHandler("rag_system.log"),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger("RAG")


logger = init_logger(logging.INFO)


def md5_text(text: str) -> str:
    return hashlib.md5(text.encode("utf-8", "ignore")).hexdigest()


def compute_documents_hash(documents: List[Any]) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    hasher = hashlib.md5()
    for doc in sorted(documents, key=lambda x: x.metadata.get('source', '')):
        content = doc.page_content.encode('utf-8', errors='ignore')
        hasher.update(content)
    return hasher.hexdigest()


# -----------------------------------------------------------------------------
# 3. ‚úÖ –ù–û–í–´–ô ColBERT Reranker
# -----------------------------------------------------------------------------

class ColBERTReranker:
    """ColBERT reranker –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""

    def __init__(self, cfg: RAGConfig):
        self.cfg = cfg
        self.model: Optional[RAGPretrainedModel] = None
        self._available = COLBERT_AVAILABLE

    @property
    def colbert_model(self) -> Optional[RAGPretrainedModel]:
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ColBERT –º–æ–¥–µ–ª–∏"""
        if not self._available:
            return None

        if self.model is None:
            try:
                logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ ColBERT –º–æ–¥–µ–ª–∏: {self.cfg.colbert_model}")
                self.model = RAGPretrainedModel.from_pretrained(self.cfg.colbert_model)
                logger.info("‚úÖ ColBERT –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ColBERT: {e}")
                self._available = False
                return None
        return self.model

    def rerank(self, query: str, documents: List[Any], k: int = 10) -> List[Any]:
        """–ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é ColBERT"""
        if not self._available or not documents:
            logger.debug("ColBERT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è")
            return documents[:k]

        model = self.colbert_model
        if model is None:
            return documents[:k]

        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            doc_texts = []
            for doc in documents:
                if hasattr(doc, 'page_content'):
                    doc_texts.append(doc.page_content)
                else:
                    doc_texts.append(str(doc))

            if not doc_texts:
                return documents[:k]

            # –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å ColBERT
            start_time = time.time()
            ranked_results = model.rerank(query=query, documents=doc_texts, k=k)
            rerank_time = time.time() - start_time

            logger.debug(f"ColBERT –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: {len(documents)} -> {len(ranked_results)} –∑–∞ {rerank_time:.2f}—Å")

            # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            reranked_docs = []
            for result in ranked_results:
                # RAGatouille –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å 'content' –∏ 'score'
                if isinstance(result, dict) and 'content' in result:
                    content = result['content']
                    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
                    for doc in documents:
                        doc_content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                        if doc_content == content:
                            reranked_docs.append(doc)
                            break
                else:
                    # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    reranked_docs.append(result)

            return reranked_docs[:k]

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ ColBERT —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return documents[:k]


# -----------------------------------------------------------------------------
# 4. ‚úÖ CONTEXT-AWARE Cache Manager
# -----------------------------------------------------------------------------

class ContextAwareCacheManager:
    def __init__(self, ttl: int):
        self.cache = TTLCache(maxsize=1_000, ttl=ttl)
        self.user_contexts = {}  # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        self.hits = 0
        self.misses = 0

    def _generate_context_aware_key(self, query: str, user_id: str = None, context_window: int = 3) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫–ª—é—á –∫—ç—à–∞ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # –ë–∞–∑–æ–≤—ã–π —Ö—ç—à –∑–∞–ø—Ä–æ—Å–∞
        base_hash = md5_text(query.lower().strip())

        if user_id:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            user_history = self.user_contexts.get(user_id, [])
            recent_queries = user_history[-context_window:]
            context_hash = md5_text("|".join(recent_queries))
            return f"{base_hash}_{context_hash}_{user_id}"

        return base_hash

    def get(self, query: str, user_id: str = None) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        cache_key = self._generate_context_aware_key(query, user_id)
        result = self.cache.get(cache_key)

        if result:
            self.hits += 1
            logger.debug(f"Cache hit –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {query[:50]}...")
        else:
            self.misses += 1

        return result

    def set(self, query: str, answer: str, user_id: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        cache_key = self._generate_context_aware_key(query, user_id)
        self.cache[cache_key] = answer

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_id:
            if user_id not in self.user_contexts:
                self.user_contexts[user_id] = []

            self.user_contexts[user_id].append(query.lower().strip())

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é (—Ö—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –∑–∞–ø—Ä–æ—Å–æ–≤)
            if len(self.user_contexts[user_id]) > 10:
                self.user_contexts[user_id] = self.user_contexts[user_id][-10:]

    def clear_user_context(self, user_id: str):
        """–û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if user_id in self.user_contexts:
            del self.user_contexts[user_id]

        # –£–¥–∞–ª—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫—ç—à-–∑–∞–ø–∏—Å–∏
        keys_to_remove = [k for k in self.cache.keys() if str(user_id) in str(k)]
        for key in keys_to_remove:
            del self.cache[key]

    def get_stats(self) -> dict:
        total = self.hits + self.misses
        hit_rate = (self.hits / max(total, 1)) * 100
        return {
            "cache_hits": self.hits,
            "cache_misses": self.misses,
            "hit_rate": f"{hit_rate:.2f}%"
        }


# -----------------------------------------------------------------------------
# 5. ‚úÖ DYNAMIC LLM Manager
# -----------------------------------------------------------------------------

class DynamicLLMManager:
    def __init__(self, base_llm):
        self.base_llm = base_llm
        self.response_history = {}  # user_id -> –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—Ç–≤–µ—Ç—ã

    def _calculate_response_similarity(self, new_response: str, user_id: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if user_id not in self.response_history:
            return 0.0

        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é embeddings)
        previous_responses = self.response_history[user_id]
        max_similarity = 0.0

        for prev_response in previous_responses:
            # Jaccard similarity –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏
            set1 = set(new_response.lower().split())
            set2 = set(prev_response.lower().split())
            similarity = len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0
            max_similarity = max(max_similarity, similarity)

        return max_similarity

    def generate_response(self, prompt: str, user_id: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        temperature = 0.4
        top_p = 0.85
        top_k = 40

        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        if user_id and user_id in self.response_history:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –∏—Å—Ç–æ—Ä–∏–µ–π
            temperature = min(0.7, 0.4 + len(self.response_history[user_id]) * 0.05)
            top_p = min(0.95, 0.85 + len(self.response_history[user_id]) * 0.02)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ prompt
        enhanced_prompt = f"""{prompt}

–í–ê–ñ–ù–û: –î–∞–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π, —Å–≤–µ–∂–∏–π –æ—Ç–≤–µ—Ç. –ò–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è —Ñ—Ä–∞–∑ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–π —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏ –ø–æ–¥—Ö–æ–¥—ã –∫ –æ–±—ä—è—Å–Ω–µ–Ω–∏—é."""

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        original_temp = getattr(self.base_llm, 'temperature', 0.4)
        original_top_p = getattr(self.base_llm, 'top_p', 0.85)
        original_top_k = getattr(self.base_llm, 'top_k', 40)

        self.base_llm.temperature = temperature
        self.base_llm.top_p = top_p
        self.base_llm.top_k = top_k

        response = self.base_llm.invoke(enhanced_prompt)

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.base_llm.temperature = original_temp
        self.base_llm.top_p = original_top_p
        self.base_llm.top_k = original_top_k

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å—Ö–æ–¥—Å—Ç–≤–æ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º
        if user_id:
            similarity = self._calculate_response_similarity(response, user_id)

            if similarity > 0.7:  # –°–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏–π –æ—Ç–≤–µ—Ç
                logger.info(f"–í—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ ({similarity:.2f}), —Ä–µ–≥–µ–Ω–µ—Ä–∏—Ä—É—é —Å –±–æ–ª—å—à–µ–π –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é")

                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è —Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                self.base_llm.temperature = min(0.9, temperature + 0.3)
                self.base_llm.top_p = 0.95

                creative_prompt = f"""{prompt}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç –±—ã–ª —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂ –Ω–∞ —É–∂–µ –¥–∞–Ω–Ω—ã–µ. 
–î–∞–π –°–û–í–ï–†–®–ï–ù–ù–û –î–†–£–ì–û–ô –æ—Ç–≤–µ—Ç —Å:
- –î—Ä—É–≥–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏–∑–ª–æ–∂–µ–Ω–∏—è
- –î—Ä—É–≥–∏–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ –∞–Ω–∞–ª–æ–≥–∏—è–º–∏  
- –î—Ä—É–≥–∏–º —Å—Ç–∏–ª–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
- –î—Ä—É–≥–∏–º–∏ –∞–∫—Ü–µ–Ω—Ç–∞–º–∏ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏"""

                response = self.base_llm.invoke(creative_prompt)

                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                self.base_llm.temperature = original_temp
                self.base_llm.top_p = original_top_p

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
        self._update_response_history(response, user_id)

        return response.strip()

    def _update_response_history(self, response: str, user_id: str):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if not user_id:
            return

        if user_id not in self.response_history:
            self.response_history[user_id] = []

        self.response_history[user_id].append(response)

        # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –æ—Ç–≤–µ—Ç–æ–≤
        if len(self.response_history[user_id]) > 5:
            self.response_history[user_id] = self.response_history[user_id][-5:]

    def clear_user_history(self, user_id: str):
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if user_id in self.response_history:
            del self.response_history[user_id]


# -----------------------------------------------------------------------------
# 6. ‚úÖ –£–õ–£–ß–®–ï–ù–ù–´–ô –ú–µ–Ω–µ–¥–∂–µ—Ä —Ä–µ—Å—É—Ä—Å–æ–≤ —Å ColBERT
# -----------------------------------------------------------------------------

class ResourceManager:
    """–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –∑–∞–≥—Ä—É–∑–∫—É/–æ—á–∏—Å—Ç–∫—É —Ç—è–∂—ë–ª—ã—Ö –º–æ–¥–µ–ª–µ–π."""

    def __init__(self, cfg: RAGConfig):
        self.cfg = cfg
        self._emb_model: Optional[HuggingFaceEmbeddings] = None
        self._sentence_embedder: Optional[HuggingFaceEmbeddings] = None
        self._colbert_reranker: Optional[ColBERTReranker] = None
        self._llm: Optional[OllamaLLM] = None

    @property
    def emb_model(self) -> HuggingFaceEmbeddings:
        """–ú–æ–¥–µ–ª—å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ"""
        if self._emb_model is None:
            try:
                self._emb_model = HuggingFaceEmbeddings(
                    model_name="Qwen/Qwen3-Embedding-0.6B",
                    model_kwargs={"device": self.cfg.device},
                    encode_kwargs={
                        "batch_size": self.cfg.batch_size,
                        "normalize_embeddings": True
                    }
                )
                logger.info("Embedding-–º–æ–¥–µ–ª—å –¥–ª—è FAISS –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–ª–µ–Ω–∏–≤–æ): Qwen/Qwen3-Embedding-0.6B")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ embedding –º–æ–¥–µ–ª–∏: {e}")
                raise
        return self._emb_model

    @property
    def sentence_embedder(self) -> HuggingFaceEmbeddings:
        """–ú–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞"""
        if self._sentence_embedder is None:
            try:
                self._sentence_embedder = HuggingFaceEmbeddings(
                    model_name="sberbank-ai/sbert_large_nlu_ru",
                    model_kwargs={'device': self.cfg.device},
                    encode_kwargs={
                        'batch_size': self.cfg.batch_size,
                        'normalize_embeddings': True
                    }
                )
                logger.info("Embedding-–º–æ–¥–µ–ª—å –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–ª–µ–Ω–∏–≤–æ): sberbank-ai/sbert_large_nlu_ru")
            except Exception as e:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞: {e}. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω RecursiveCharacterTextSplitter")
                self._sentence_embedder = None
        return self._sentence_embedder

    @property
    def colbert_reranker(self) -> ColBERTReranker:
        """ColBERT reranker –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if self._colbert_reranker is None:
            self._colbert_reranker = ColBERTReranker(self.cfg)
        return self._colbert_reranker

    @property
    def llm(self) -> OllamaLLM:
        if self._llm is None:
            try:
                # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò LLM –î–õ–Ø –ü–†–ï–î–û–¢–í–†–ê–©–ï–ù–ò–Ø –ü–û–í–¢–û–†–ï–ù–ò–ô
                self._llm = OllamaLLM(
                    model="qwen2.5vl:7b",
                    # ‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                    num_ctx=16384,  # –ë–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    num_predict=self.cfg.llm_num_predict,  # –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
                    temperature=self.cfg.llm_temperature,  # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                    repeat_penalty=self.cfg.llm_repeat_penalty,  # –ú—è–≥–∫–æ–µ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ
                    repeat_last_n=self.cfg.llm_repeat_last_n,  # –û–∫–Ω–æ –∞–Ω–∞–ª–∏–∑–∞
                    top_p=self.cfg.llm_top_p,  # Nucleus sampling
                    top_k=self.cfg.llm_top_k,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
                    # ‚úÖ –°—Ç–æ–ø-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    stop=self.cfg.llm_stop_sequences,
                    # ‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                    seed=42,  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –¥–ª—è –±–æ–ª—å—à–µ–π —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏)
                )
                logger.info("‚úÖ LLM –∑–∞–≥—Ä—É–∂–µ–Ω —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø—Ä–æ—Ç–∏–≤ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LLM: {e}")
                raise
        return self._llm

    @staticmethod
    def gpu_clear():
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.debug("GPU-cache –æ—á–∏—â–µ–Ω")


# -----------------------------------------------------------------------------
# 7. –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π PDF, DOCX, TXT
# -----------------------------------------------------------------------------

class AdvancedDocumentLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π PDF, DOCX –∏ TXT —Ñ–∞–π–ª–æ–≤."""
    SUPPORTED_EXTENSIONS = {'.pdf', '.docx', '.txt'}

    def __init__(self):
        self.documents = []
        self.chunks = []

    def load_documents(self, folder_path: str) -> List[Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–∏."""
        documents = []
        if not os.path.exists(folder_path):
            logger.error(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
            raise FileNotFoundError(f"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")

        logger.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ –∏–∑ {folder_path}")
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            ext = Path(filename).suffix.lower()

            if ext not in self.SUPPORTED_EXTENSIONS:
                logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª —Å –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º: {filename}")
                continue

            try:
                loader = self._get_loader(file_path, ext)
                loaded_docs = loader.load()

                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                for doc in loaded_docs:
                    doc.metadata.update({
                        'filename': filename,
                        'file_path': file_path,
                        'file_type': ext
                    })

                documents.extend(loaded_docs)
                logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {filename} (—á–∞—Å—Ç–µ–π: {len(loaded_docs)})")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filename}: {str(e)}")
                continue

        logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
        self.documents = documents
        return documents

    def _get_loader(self, file_path: str, ext: str):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–ª—è —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞."""
        if ext == '.pdf':
            return UnstructuredPDFLoader(file_path)
        elif ext == '.docx':
            return UnstructuredWordDocumentLoader(file_path)
        elif ext == '.txt':
            return TextLoader(file_path, encoding='utf-8')
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {ext}")

    def chunk_documents(self, documents: List[Any], cfg: RAGConfig, rm: ResourceManager) -> List[Any]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏."""
        if cfg.use_semantic_chunking and rm.sentence_embedder is not None:
            chunks = self._semantic_chunking(documents, rm.sentence_embedder)
        else:
            chunks = self._recursive_chunking(documents, cfg)

        # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –¥–ª—è —á–∞–Ω–∫–æ–≤
        for i, chunk in enumerate(chunks):
            chunk.metadata["chunk_id"] = i
            chunk.metadata["original_doc_count"] = len(documents)

        self.chunks = chunks
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        return chunks

    def _semantic_chunking(self, documents: List[Any], sentence_embedder: HuggingFaceEmbeddings) -> List[Any]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏."""
        try:
            chunker = SemanticChunker(
                embeddings=sentence_embedder,
                breakpoint_threshold_type="percentile",
                breakpoint_threshold_amount=0.5
            )
            chunks = chunker.split_documents(documents)
            logger.info(f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            return chunks
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è: {e}. –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ RecursiveCharacterTextSplitter")
            return self._recursive_chunking(documents, None)

    def _recursive_chunking(self, documents: List[Any], cfg: Optional[RAGConfig]) -> List[Any]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏."""
        chunk_size = cfg.chunk_size if cfg else 1000
        chunk_overlap = cfg.chunk_overlap if cfg else 200

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", "! ", "? ", "; ", " ", ""]
        )
        chunks = splitter.split_documents(documents)
        logger.info(f"–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks


# -----------------------------------------------------------------------------
# 8. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ FAISS-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
# -----------------------------------------------------------------------------

class OptimizedFaissStore:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ FAISS-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è."""

    def __init__(self, cfg: RAGConfig, rm: ResourceManager):
        self.cfg = cfg
        self.rm = rm
        self.vector_store: Optional[FAISS] = None
        self.documents_hash: Optional[str] = None
        self._ready = asyncio.Event()

    async def build(self, chunks: List[Any]):
        """–°—Ç—Ä–æ–∏–º –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ."""
        current_hash = compute_documents_hash(chunks)
        hash_file = os.path.join(os.path.dirname(self.cfg.faiss_path), "documents_hash.json")

        force_recreate = True
        if os.path.exists(hash_file) and os.path.exists(self.cfg.faiss_path):
            try:
                with open(hash_file, 'r', encoding='utf-8') as f:
                    saved_data = json.load(f)
                saved_hash = saved_data.get('hash')
                if saved_hash == current_hash:
                    force_recreate = False
                    logger.info(f"–•—ç—à –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è: {current_hash}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ —Ö—ç—à–∞: {e}")

        if force_recreate:
            await self._create_new_index(chunks, current_hash, hash_file)
        else:
            await self._load_existing_index()

        self._ready.set()

    async def _create_new_index(self, chunks: List[Any], current_hash: str, hash_file: str):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π FAISS –∏–Ω–¥–µ–∫—Å."""
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        self.rm.gpu_clear()

        try:
            self.vector_store = FAISS.from_documents(chunks, self.rm.emb_model)
            os.makedirs(os.path.dirname(self.cfg.faiss_path), exist_ok=True)
            self.vector_store.save_local(self.cfg.faiss_path)

            with open(hash_file, 'w', encoding='utf-8') as f:
                json.dump({'hash': current_hash}, f)

            self.documents_hash = current_hash
            logger.info(f"FAISS –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {self.cfg.faiss_path}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise

    async def _load_existing_index(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π FAISS –∏–Ω–¥–µ–∫—Å."""
        try:
            self.rm.gpu_clear()
            self.vector_store = FAISS.load_local(
                self.cfg.faiss_path,
                self.rm.emb_model,
                allow_dangerous_deserialization=True
            )
            logger.info(f"FAISS –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {self.cfg.faiss_path}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ FAISS –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise

    async def search(self, query: str, k: int = 10) -> List[Any]:
        """–ü–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É."""
        await self._ready.wait()
        if not self.vector_store:
            logger.warning("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
            return []

        try:
            docs = self.vector_store.similarity_search(query, k=k)
            return docs
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ FAISS: {e}")
            return []


# -----------------------------------------------------------------------------
# 9. BM25-—Ä–µ—Ç—Ä–∏–≤–µ—Ä (async –æ–±–æ–ª–æ—á–∫–∞)
# -----------------------------------------------------------------------------

class AsyncBM25Retriever:
    def __init__(self, chunks: List[Any]):
        self.retriever = BM25Retriever.from_documents(chunks)
        self.retriever.k = 10
        logger.info("BM25Retriever –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def retrieve(self, query: str, k: int = 10) -> List[Any]:
        loop = asyncio.get_running_loop()
        try:
            self.retriever.k = k
            docs = await loop.run_in_executor(None, self.retriever.invoke, query)
            return docs
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ BM25 –ø–æ–∏—Å–∫–∞: {e}")
            return []


# -----------------------------------------------------------------------------
# 10. ‚úÖ GraphRAGSystem –° –°–û–•–†–ê–ù–ï–ù–ò–ï–ú –ò –ó–ê–ì–†–£–ó–ö–û–ô –°–¢–†–£–ö–¢–£–†–´ –õ–ï–ô–î–ï–ù–ê
# -----------------------------------------------------------------------------

class GraphRAGSystem:
    """‚úÖ –ü–û–õ–ù–ê–Ø GraphRAG —Å–∏—Å—Ç–µ–º–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏ –∑–∞–≥—Ä—É–∑–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –õ–µ–π–¥–µ–Ω–∞."""

    def __init__(self, cfg: RAGConfig, rm: ResourceManager):
        self.cfg, self.rm = cfg, rm
        self.graph_driver = None
        self.chunks: List[Any] = []
        self.community_summaries: dict = {}

        # ‚úÖ –ù–û–í–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º/–∑–∞–≥—Ä—É–∑–∫–æ–π
        self.graph_metadata_file = os.path.join(cfg.graph_metadata_dir, "graph_metadata.json")
        self.community_summaries_file = os.path.join(cfg.graph_metadata_dir, "community_summaries.json")
        self.graph_built = False

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        os.makedirs(cfg.graph_metadata_dir, exist_ok=True)

        self._entity_extraction_prompt = """
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–∞. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—É—â–Ω–æ—Å—Ç–∏ –∏ —Å–≤—è–∑–∏.

–¢–ï–ö–°–¢:
{text}

–ò–ù–°–¢–†–£–ö–¶–ò–Ø: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤:

{{
  "entities": [
    {{"text": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "label": "—Ç–∏–ø", "description": "–æ–ø–∏—Å–∞–Ω–∏–µ"}}
  ],
  "relations": [
    {{"source": "—Å—É—â–Ω–æ—Å—Ç—å1", "target": "—Å—É—â–Ω–æ—Å—Ç—å2", "type": "—Å–≤—è–∑—å", "description": "–æ–ø–∏—Å–∞–Ω–∏–µ"}}
  ]
}}

–ï—Å–ª–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–µ—Ç, –≤–µ—Ä–Ω–∏: {{"entities": [], "relations": []}}
"""

        self._query_entity_extraction_prompt = """
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –∫–ª—é—á–µ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
–ò–∑–≤–ª–µ–∫–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–º–æ—á—å –≤ –ø–æ–∏—Å–∫–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–ó–∞–ø—Ä–æ—Å: {query}

–í–µ—Ä–Ω–∏ JSON-–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π:
["—Å—É—â–Ω–æ—Å—Ç—å1", "—Å—É—â–Ω–æ—Å—Ç—å2", ...]
"""

        try:
            self.graph_driver = AsyncGraphDatabase.driver(
                cfg.neo4j_uri,
                auth=(cfg.neo4j_user, cfg.neo4j_password),
                max_connection_pool_size=10,
                connection_timeout=30
            )
            logger.info("GraphRAG: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ Neo4j —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Neo4j: {e}")
            self.graph_driver = None

    async def verify_connection(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Neo4j."""
        if not self.graph_driver:
            return False

        try:
            await self.graph_driver.verify_connectivity()
            return True
        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {e}")
            return False

    async def _force_clear_database(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –≥—Ä–∞—Ñ–æ–≤–æ–π –ë–î."""
        try:
            asyncio.get_running_loop()
        except RuntimeError:
            logger.error("GraphRAG: –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ event loop –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ë–î")
            return

        async with self.graph_driver.session() as session:
            try:
                await session.run("MATCH (n) DETACH DELETE n")

                # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —É–¥–∞–ª–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –¥–ª—è Neo4j 5.x
                try:
                    await session.run("DROP CONSTRAINT entity_name_unique IF EXISTS")
                except Exception as e:
                    logger.debug(f"GraphRAG: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —É–∂–µ —É–¥–∞–ª–µ–Ω—ã: {e}")

                logger.info("GraphRAG: –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ—á–∏—â–µ–Ω–∞")

            except Exception as e:
                logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –ë–î: {e}")
                raise

    async def initialize_graph(self, chunks: List[Any]) -> bool:
        """‚úÖ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≥—Ä–∞—Ñ–∞."""
        if not await self.verify_connection():
            logger.warning("GraphRAG: Neo4j –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return False

        self.chunks = chunks

        try:
            # ‚úÖ –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –≥—Ä–∞—Ñ
            if not self.cfg.force_rebuild_graph and self.cfg.auto_load_existing:
                existing_graph = await self._check_existing_complete_graph()

                if existing_graph:
                    logger.info("üéØ GraphRAG: –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –≥—Ä–∞—Ñ —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –õ–µ–π–¥–µ–Ω–∞")
                    success = await self._load_existing_graph_structure()
                    if success:
                        logger.info("‚úÖ GraphRAG: –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –≥—Ä–∞—Ñ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
                        return True

            # –ï—Å–ª–∏ –Ω–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≥—Ä–∞—Ñ–∞ –∏–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞
            if self.cfg.force_rebuild_graph:
                logger.info("GraphRAG: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –≥—Ä–∞—Ñ–∞")
                await self._force_clear_database()

            success = await self._build_graph_from_chunks(chunks)
            if success and self.cfg.save_graph_structure:
                # ‚úÖ –°–û–•–†–ê–ù–Ø–ï–ú –≥—Ä–∞—Ñ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è
                await self._save_graph_structure()
                logger.info("GraphRAG: –ì—Ä–∞—Ñ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            return success

        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            return False

    async def _check_existing_complete_graph(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞ —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –õ–µ–π–¥–µ–Ω–∞."""
        async with self.graph_driver.session() as session:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —É–∑–ª–æ–≤ –∏ —Å–≤—è–∑–µ–π
                result = await session.run("""
                    MATCH (e:Entity) 
                    OPTIONAL MATCH ()-[r:RELATED]->()
                    RETURN count(DISTINCT e) AS nodes, count(r) AS relationships
                """)

                record = await result.single()
                if not record or record["nodes"] == 0:
                    logger.info("GraphRAG: –ì—Ä–∞—Ñ –ø—É—Å—Ç")
                    return False

                nodes_count = record["nodes"]
                rels_count = record["relationships"]
                logger.info(f"GraphRAG: –ù–∞–π–¥–µ–Ω –≥—Ä–∞—Ñ: {nodes_count} —É–∑–ª–æ–≤, {rels_count} —Å–≤—è–∑–µ–π")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è —Å–æ–æ–±—â–µ—Å—Ç–≤ –õ–µ–π–¥–µ–Ω–∞
                for level in range(6):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Ä–æ–≤–Ω–∏ 0-5
                    community_check = await session.run(f"""
                        MATCH (e:Entity) 
                        WHERE e.community_{level} IS NOT NULL
                        RETURN count(e) AS with_communities
                        LIMIT 1
                    """)

                    community_record = await community_check.single()
                    if community_record and community_record["with_communities"] > 0:
                        logger.info(f"GraphRAG: –ù–∞–π–¥–µ–Ω—ã —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ –õ–µ–π–¥–µ–Ω–∞ —É—Ä–æ–≤–Ω—è {level}")
                        return True

                logger.info("GraphRAG: –ì—Ä–∞—Ñ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –±–µ–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –õ–µ–π–¥–µ–Ω–∞")
                return False

            except Exception as e:
                logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –≥—Ä–∞—Ñ–∞: {e}")
                return False

    async def _load_existing_graph_structure(self) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≥—Ä–∞—Ñ–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ."""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∞
            await self._load_graph_metadata()

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–æ–¥–∫–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤
            await self._load_community_summaries_from_file()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ —É—Ä–æ–≤–Ω–∏ –õ–µ–π–¥–µ–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω—ã
            available_levels = await self._get_available_leiden_levels()
            logger.info(f"GraphRAG: –î–æ—Å—Ç—É–ø–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –õ–µ–π–¥–µ–Ω–∞: {available_levels}")

            if available_levels:
                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —É—Ä–æ–≤–Ω–µ–π
                await self._compute_community_metrics_safe(available_levels)
                logger.info("GraphRAG: ‚úÖ –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –õ–µ–π–¥–µ–Ω–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return True
            else:
                logger.warning("GraphRAG: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –õ–µ–π–¥–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False

        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
            return False

    async def _get_available_leiden_levels(self) -> List[int]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–µ–π–¥–µ–Ω–∞."""
        available_levels = []

        async with self.graph_driver.session() as session:
            try:
                for level in range(6):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Ä–æ–≤–Ω–∏ 0-5
                    result = await session.run(f"""
                        MATCH (e:Entity) 
                        WHERE e.community_{level} IS NOT NULL
                        RETURN count(e) AS count
                        LIMIT 1
                    """)

                    record = await result.single()
                    if record and record["count"] > 0:
                        available_levels.append(level)

            except Exception as e:
                logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É—Ä–æ–≤–Ω–µ–π –õ–µ–π–¥–µ–Ω–∞: {e}")

        return available_levels

    async def _save_graph_structure(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≥—Ä–∞—Ñ–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ."""
        try:
            # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∞
            await self._save_graph_metadata()

            # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤ –≤ —Ñ–∞–π–ª
            await self._save_community_summaries_to_file()

            # 3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–µ–π–¥–µ–Ω–∞ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∫–∞–∫ —Å–≤–æ–π—Å—Ç–≤–∞ —É–∑–ª–æ–≤
            logger.info("GraphRAG: ‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≥—Ä–∞—Ñ–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")

    async def _save_graph_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∞ –≤ —Ñ–∞–π–ª."""
        try:
            async with self.graph_driver.session() as session:
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä–∞—Ñ–∞
                stats_result = await session.run("""
                    MATCH (e:Entity) 
                    OPTIONAL MATCH ()-[r:RELATED]->()
                    RETURN count(DISTINCT e) AS nodes, count(r) AS relationships
                """)

                stats_record = await stats_result.single()

                # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—Ä–æ–≤–Ω—è—Ö –õ–µ–π–¥–µ–Ω–∞
                leiden_levels = {}
                for level in range(6):
                    level_result = await session.run(f"""
                        MATCH (e:Entity) 
                        WHERE e.community_{level} IS NOT NULL
                        WITH e.community_{level} AS community, count(e) AS size
                        RETURN count(DISTINCT community) AS communities, 
                               avg(size) AS avg_size, 
                               max(size) AS max_size,
                               min(size) AS min_size
                    """)

                    level_record = await level_result.single()
                    if level_record and level_record["communities"]:
                        leiden_levels[level] = {
                            "communities": level_record["communities"],
                            "avg_size": float(level_record["avg_size"]),
                            "max_size": level_record["max_size"],
                            "min_size": level_record["min_size"]
                        }

                metadata = {
                    "created_timestamp": time.time(),
                    "nodes_count": stats_record["nodes"],
                    "relationships_count": stats_record["relationships"],
                    "leiden_levels": leiden_levels,
                    "chunks_processed": len(self.chunks),
                    "version": "1.0"
                }

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
                with open(self.graph_metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)

                logger.info(f"GraphRAG: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.graph_metadata_file}")

        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

    async def _load_graph_metadata(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–∞ –∏–∑ —Ñ–∞–π–ª–∞."""
        try:
            if os.path.exists(self.graph_metadata_file):
                with open(self.graph_metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                logger.info(f"GraphRAG: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {self.graph_metadata_file}")
                logger.info(
                    f"GraphRAG: –ì—Ä–∞—Ñ —Å–æ–∑–¥–∞–Ω: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(metadata['created_timestamp']))}")
                logger.info(f"GraphRAG: –£–∑–ª–æ–≤: {metadata['nodes_count']}, —Å–≤—è–∑–µ–π: {metadata['relationships_count']}")
                logger.info(f"GraphRAG: –£—Ä–æ–≤–Ω–∏ –õ–µ–π–¥–µ–Ω–∞: {list(metadata['leiden_levels'].keys())}")
                return metadata
            else:
                logger.debug(f"GraphRAG: –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö {self.graph_metadata_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None

        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return None

    async def _save_community_summaries_to_file(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–æ–¥–∫–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤ –≤ —Ñ–∞–π–ª."""
        try:
            if self.community_summaries:
                with open(self.community_summaries_file, 'w', encoding='utf-8') as f:
                    json.dump(self.community_summaries, f, indent=2, ensure_ascii=False)

                logger.info(f"GraphRAG: –°–≤–æ–¥–∫–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.community_summaries_file}")
            else:
                logger.debug("GraphRAG: –ù–µ—Ç —Å–≤–æ–¥–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")

        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤–æ–¥–æ–∫: {e}")

    async def _load_community_summaries_from_file(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–≤–æ–¥–∫–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤ –∏–∑ —Ñ–∞–π–ª–∞."""
        try:
            if os.path.exists(self.community_summaries_file):
                with open(self.community_summaries_file, 'r', encoding='utf-8') as f:
                    self.community_summaries = json.load(f)

                logger.info(f"GraphRAG: –°–≤–æ–¥–∫–∏ —Å–æ–æ–±—â–µ—Å—Ç–≤ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ {self.community_summaries_file}")
            else:
                logger.debug(f"GraphRAG: –§–∞–π–ª —Å–≤–æ–¥–æ–∫ {self.community_summaries_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                self.community_summaries = {}

        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–≤–æ–¥–æ–∫: {e}")
            self.community_summaries = {}

    async def _build_graph_from_chunks(self, chunks: List[Any]) -> bool:
        """‚úÖ –ú–ê–°–®–¢–ê–ë–ò–†–£–ï–ú–û–ï –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π –∏–∑ –í–°–ï–• —á–∞–Ω–∫–æ–≤."""
        all_entities: dict = {}
        all_relations: List[dict] = []

        logger.info(f"GraphRAG: –û–±—Ä–∞–±–æ—Ç–∫–∞ –í–°–ï–• {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π...")

        successful_extractions = 0
        batch_size = 100  # ‚úÖ –£–í–ï–õ–ò–ß–ï–ù–ù–´–ô —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤

        for batch_start in range(0, len(chunks), batch_size):
            batch_end = min(batch_start + batch_size, len(chunks))
            batch_chunks = chunks[batch_start:batch_end]

            logger.info(
                f"GraphRAG: –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_start}-{batch_end}/{len(chunks)} ({((batch_end / len(chunks)) * 100):.1f}%)")

            for i, chunk in enumerate(batch_chunks):
                actual_i = batch_start + i

                try:
                    text = chunk.page_content[:1000]

                    prompt = self._entity_extraction_prompt.format(text=text)
                    raw_response = self.rm.llm.invoke(prompt)
                    cleaned_response = self._clean_json_response(raw_response)

                    if cleaned_response:
                        data = json.loads(cleaned_response)
                        successful_extractions += 1
                    else:
                        data = self._simple_entity_extraction(text, actual_i)

                    for entity in data.get("entities", []):
                        if not entity.get("text"):
                            continue

                        entity_key = entity["text"].lower().strip()
                        if entity_key not in all_entities:
                            entity["doc_ids"] = {actual_i}
                            all_entities[entity_key] = entity
                        else:
                            all_entities[entity_key]["doc_ids"].add(actual_i)

                    for relation in data.get("relations", []):
                        if not relation.get("source") or not relation.get("target"):
                            continue
                        relation["doc_id"] = actual_i
                        all_relations.append(relation)

                except json.JSONDecodeError as e:
                    logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –¥–ª—è —á–∞–Ω–∫–∞ {actual_i}: {e}")
                    try:
                        fallback_data = self._simple_entity_extraction(chunk.page_content[:1000], actual_i)
                        for entity in fallback_data.get("entities", []):
                            entity_key = entity["text"].lower().strip()
                            if entity_key not in all_entities:
                                entity["doc_ids"] = {actual_i}
                                all_entities[entity_key] = entity
                            else:
                                all_entities[entity_key]["doc_ids"].add(actual_i)
                    except Exception:
                        pass
                    continue

                except Exception as e:
                    logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞ {actual_i}: {e}")
                    continue

        logger.info(f"GraphRAG: –£—Å–ø–µ—à–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏–π JSON: {successful_extractions}/{len(chunks)}")
        logger.info(f"GraphRAG: –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(all_entities)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ {len(all_relations)} —Å–≤—è–∑–µ–π")

        if not all_entities:
            logger.warning("GraphRAG: –ù–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏")
            return False

        try:
            await self._create_graph_in_neo4j_with_weights(all_entities, all_relations)
            logger.info("GraphRAG: –ì—Ä–∞—Ñ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –≤ Neo4j")
        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞: {e}")
            return False

        # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º –õ–µ–π–¥–µ–Ω–∞ –¥–ª—è GDS 2.20
        try:
            await self._create_leiden_communities_gds_2_20()
            logger.info("GraphRAG: –ê–ª–≥–æ—Ä–∏—Ç–º –õ–µ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω")
        except Exception as e:
            logger.warning(f"GraphRAG: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ –õ–µ–π–¥–µ–Ω–∞: {e}")

        return True

    def _simple_entity_extraction(self, text: str, doc_id: int) -> dict:
        """–ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –±–µ–∑ LLM."""
        words = re.findall(r'\b[–ê-–Ø–Å][–∞-—è—ë]+\b|\b[A-Z][a-z]+\b', text)

        entities = []
        for word in set(words[:10]):
            if len(word) > 3:
                entities.append({
                    "text": word,
                    "label": "Concept",
                    "description": f"–°—É—â–Ω–æ—Å—Ç—å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}"
                })

        return {
            "entities": entities,
            "relations": []
        }

    def _clean_json_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç LLM –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON."""
        if not response or not response.strip():
            logger.debug("GraphRAG: –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç LLM")
            return ""

        text = response.strip()

        if hasattr(self, '_debug_count'):
            self._debug_count += 1
        else:
            self._debug_count = 1

        if self._debug_count <= 3:
            logger.debug(f"GraphRAG: –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç LLM #{self._debug_count}: {text[:200]}...")

        if "```" in text:
            lines = text.splitlines()
            code_lines = []
            in_code = False

            for line in lines:
                line_stripped = line.strip()
                if line_stripped.startswith("```"):
                    in_code = not in_code
                    continue
                if in_code:
                    code_lines.append(line)

            if code_lines:
                text = "\n".join(code_lines).strip()

        start_idx = text.find("{")
        end_idx = text.rfind("}") + 1

        if start_idx != -1 and end_idx > start_idx:
            text = text[start_idx:end_idx]
        else:
            start_idx = text.find("[")
            end_idx = text.rfind("]") + 1
            if start_idx != -1 and end_idx > start_idx:
                text = text[start_idx:end_idx]
            else:
                logger.debug(f"GraphRAG: –ù–µ –Ω–∞–π–¥–µ–Ω–∞ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ –æ—Ç–≤–µ—Ç–µ: {text[:100]}...")
                return ""

        text = text.replace("``````", "").strip()

        try:
            json.loads(text)
            return text
        except json.JSONDecodeError:
            logger.debug(f"GraphRAG: –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {text[:100]}...")
            return ""

    async def _create_graph_in_neo4j_with_weights(self, entities: dict, relations: List[dict]):
        """‚úÖ –ú–ê–°–®–¢–ê–ë–ò–†–£–ï–ú–û–ï —Å–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –≤ Neo4j —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ —Å–≤—è–∑–µ–π –¥–ª—è GDS 2.20."""
        try:
            asyncio.get_running_loop()
        except RuntimeError:
            logger.error("GraphRAG: –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ event loop –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞")
            return

        if not self.graph_driver:
            logger.error("GraphRAG: Neo4j –¥—Ä–∞–π–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return

        try:
            async with self.graph_driver.session() as session:
                # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —Å–æ–∑–¥–∞–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –¥–ª—è Neo4j 5.x
                try:
                    await session.run(
                        "CREATE CONSTRAINT entity_name_unique IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE")
                    logger.info("GraphRAG: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω–æ")
                except Exception as e:
                    logger.warning(f"GraphRAG: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {e}")

                logger.info("GraphRAG: –°–æ–∑–¥–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ Neo4j...")
                entity_list = list(entities.values())
                batch_size = 200  # ‚úÖ –£–í–ï–õ–ò–ß–ï–ù–ù–´–ô batch_size –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤

                for i in range(0, len(entity_list), batch_size):
                    batch = entity_list[i:i + batch_size]
                    entity_batch = []

                    for entity in batch:
                        entity_batch.append({
                            "name": str(entity["text"])[:100],
                            "label": str(entity.get("label", "Unknown"))[:50],
                            "description": str(entity.get("description", ""))[:200],
                            "doc_ids": list(entity["doc_ids"])
                        })

                    try:
                        await session.run(
                            "UNWIND $batch AS entity "
                            "CREATE (e:Entity {name: entity.name, label: entity.label, description: entity.description, doc_ids: entity.doc_ids})",
                            batch=entity_batch
                        )
                        logger.info(
                            f"GraphRAG: –°–æ–∑–¥–∞–Ω –±–∞—Ç—á —Å—É—â–Ω–æ—Å—Ç–µ–π {i // batch_size + 1}/{(len(entity_list) - 1) // batch_size + 1}")
                    except Exception as e:
                        logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞—Ç—á–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
                        continue

                logger.info("GraphRAG: –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤—è–∑–µ–π —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –≤ Neo4j...")
                if relations:
                    for i in range(0, len(relations), batch_size):
                        batch = relations[i:i + batch_size]
                        relation_batch = []

                        for relation in batch:
                            if relation.get("source") and relation.get("target"):
                                relation_batch.append({
                                    "source": str(relation["source"])[:100],
                                    "target": str(relation["target"])[:100],
                                    "type": str(relation.get("type", "RELATED"))[:50],
                                    "description": str(relation.get("description", ""))[:200],
                                    "doc_id": relation["doc_id"],
                                    "weight": 1.0  # ‚úÖ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –≤–µ—Å –¥–ª—è GDS 2.20
                                })

                        if relation_batch:
                            try:
                                # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∑–∞–ø—Ä–æ—Å —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
                                await session.run(
                                    """
                                    UNWIND $batch AS rel
                                    MATCH (a:Entity {name: rel.source})
                                    WITH rel, a
                                    MATCH (b:Entity {name: rel.target})
                                    CREATE (a)-[:RELATED {
                                        type: rel.type, 
                                        description: rel.description, 
                                        doc_id: rel.doc_id,
                                        weight: rel.weight
                                    }]->(b)
                                    """,
                                    batch=relation_batch
                                )
                                logger.info(
                                    f"GraphRAG: –°–æ–∑–¥–∞–Ω –±–∞—Ç—á —Å–≤—è–∑–µ–π {i // batch_size + 1}/{(len(relations) - 1) // batch_size + 1}")
                            except Exception as e:
                                logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –±–∞—Ç—á–∞ —Å–≤—è–∑–µ–π: {e}")
                                continue

                logger.info("GraphRAG: –ì—Ä–∞—Ñ —Å –≤–µ—Å–∞–º–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –≤ Neo4j")

        except Exception as e:
            logger.error(f"GraphRAG: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∞: {e}")

    async def _create_leiden_communities_gds_2_20(self):
        """‚úÖ –ú–ê–°–®–¢–ê–ë–ò–†–£–ï–ú–´–ô –∞–ª–≥–æ—Ä–∏—Ç–º –õ–µ–π–¥–µ–Ω–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–∞."""
        async with self.graph_driver.session() as session:
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GDS
                try:
                    result = await session.run(
                        "CALL gds.debug.sysInfo() YIELD key, value WHERE key = 'gdsVersion' RETURN value")
                    record = await result.single()
                    if record:
                        gds_version = record["value"]
                        logger.info(f"GraphRAG: ‚úÖ GDS –¥–æ—Å—Ç—É–ø–µ–Ω, –≤–µ—Ä—Å–∏—è: {gds_version}")
                    else:
                        raise Exception("GDS version not found")
                except Exception as e:
                    logger.warning(f"GraphRAG: GDS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({e}), –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥")
                    await self._create_simple_communities()
                    return

                # ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≥—Ä–∞—Ñ–∞ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–µ–π–¥–µ–Ω–∞
                node_count_result = await session.run("MATCH (e:Entity) RETURN count(e) AS node_count")
                node_count_record = await node_count_result.single()
                node_count = node_count_record["node_count"] if node_count_record else 0

                edge_count_result = await session.run("MATCH ()-[r:RELATED]->() RETURN count(r) AS edge_count")
                edge_count_record = await edge_count_result.single()
                edge_count = edge_count_record["edge_count"] if edge_count_record else 0

                logger.info(f"GraphRAG: –†–∞–∑–º–µ—Ä –≥—Ä–∞—Ñ–∞: {node_count} —É–∑–ª–æ–≤, {edge_count} —Å–≤—è–∑–µ–π")

                # ‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–µ–π–¥–µ–Ω–∞
                if node_count < 10 or edge_count < 15:
                    logger.warning(f"GraphRAG: –ì—Ä–∞—Ñ —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–µ–π–¥–µ–Ω–∞ (–º–∏–Ω–∏–º—É–º: 10 —É–∑–ª–æ–≤, 15 —Å–≤—è–∑–µ–π). "
                                   f"–¢–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä: {node_count} —É–∑–ª–æ–≤, {edge_count} —Å–≤—è–∑–µ–π. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback.")
                    await self._create_simple_communities()
                    return

                # ‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –≥—Ä–∞—Ñ–∞ —Å –Ω–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏
                try:
                    await session.run("""
                        CALL gds.graph.project('knowledge_graph', 'Entity', {
                            RELATED: {
                                orientation: 'UNDIRECTED',
                                properties: {
                                    weight: {
                                        property: 'weight',
                                        defaultValue: 1.0
                                    }
                                }
                            }
                        })
                    """)
                    logger.info("GraphRAG: ‚úÖ –ü—Ä–æ–µ–∫—Ü–∏—è –≥—Ä–∞—Ñ–∞ —Å–æ–∑–¥–∞–Ω–∞ —Å –Ω–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏")
                except Exception as e:
                    logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ü–∏–∏ —Å –≤–µ—Å–∞–º–∏: {e}")
                    try:
                        await session.run("""
                            CALL gds.graph.project('knowledge_graph', 'Entity', {
                                RELATED: {
                                    orientation: 'UNDIRECTED'
                                }
                            })
                        """)
                        logger.info("GraphRAG: ‚úÖ –ü—Ä–æ–µ–∫—Ü–∏—è –≥—Ä–∞—Ñ–∞ —Å–æ–∑–¥–∞–Ω–∞ –Ω–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ (–±–µ–∑ –≤–µ—Å–æ–≤)")
                    except Exception as e2:
                        logger.error(f"GraphRAG: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ü–∏–∏: {e2}")
                        await self._create_simple_communities()
                        return

                # ‚úÖ –ê–ª–≥–æ—Ä–∏—Ç–º –õ–µ–π–¥–µ–Ω–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤
                gamma_values = [0.1, 0.3, 0.5, 0.8, 1.0, 1.5]
                successful_levels = []

                for level, gamma in enumerate(gamma_values):
                    try:
                        logger.info(f"GraphRAG: –ó–∞–ø—É—Å–∫ –õ–µ–π–¥–µ–Ω–∞ —É—Ä–æ–≤–µ–Ω—å {level} (gamma={gamma})...")
                        result = await session.run("""
                            CALL gds.leiden.write('knowledge_graph', {
                                writeProperty: $property,
                                gamma: $gamma,
                                maxLevels: 10,
                                tolerance: 1e-4,
                                randomSeed: 42
                            }) 
                            YIELD communityCount, modularity
                            RETURN communityCount, modularity
                        """, property=f"community_{level}", gamma=gamma)

                        record = await result.single()
                        if record:
                            successful_levels.append(level)
                            logger.info(f"GraphRAG: ‚úÖ –£—Ä–æ–≤–µ–Ω—å {level} (gamma={gamma}): "
                                        f"{record['communityCount']} —Å–æ–æ–±—â–µ—Å—Ç–≤, "
                                        f"–º–æ–¥—É–ª—å–Ω–æ—Å—Ç—å={record['modularity']:.3f}")

                    except Exception as e:
                        logger.warning(f"GraphRAG: ‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ {level} (gamma={gamma}): {e}")
                        continue

                # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                try:
                    await session.run("CALL gds.graph.drop('knowledge_graph')")
                    logger.info("GraphRAG: ‚úÖ –ü—Ä–æ–µ–∫—Ü–∏—è –≥—Ä–∞—Ñ–∞ —É–¥–∞–ª–µ–Ω–∞")
                except Exception as e:
                    logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ü–∏–∏: {e}")

                if successful_levels:
                    await self._compute_community_metrics_safe(successful_levels)
                    await self._generate_hierarchical_summaries_safe(successful_levels)
                    logger.info(f"GraphRAG: ‚úÖ –ê–ª–≥–æ—Ä–∏—Ç–º –õ–µ–π–¥–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {len(successful_levels)} —É—Ä–æ–≤–Ω–µ–π")
                else:
                    logger.warning("GraphRAG: ‚ùå –ù–∏ –æ–¥–∏–Ω —É—Ä–æ–≤–µ–Ω—å –õ–µ–π–¥–µ–Ω–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    await self._create_simple_communities()

            except Exception as e:
                logger.error(f"GraphRAG: ‚ùå –û—à–∏–±–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–µ–π–¥–µ–Ω–∞: {e}")
                await self._create_simple_communities()

    async def _compute_community_metrics_safe(self, successful_levels: List[int]):
        """‚úÖ –ë–ï–ó–û–ü–ê–°–ù–û–ï –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ - —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤–æ–π—Å—Ç–≤."""
        async with self.graph_driver.session() as session:
            try:
                for level in successful_levels:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞
                    check_result = await session.run(f"""
                        MATCH (e:Entity) 
                        WHERE e.community_{level} IS NOT NULL
                        RETURN count(e) AS count
                        LIMIT 1
                    """)

                    check_record = await check_result.single()
                    if not check_record or check_record["count"] == 0:
                        logger.debug(f"GraphRAG: –°–≤–æ–π—Å—Ç–≤–æ community_{level} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        continue

                    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    result = await session.run(f"""
                        MATCH (e:Entity) 
                        WHERE e.community_{level} IS NOT NULL
                        WITH e.community_{level} AS community, collect(e) AS entities
                        WITH community, entities, size(entities) AS community_size
                        RETURN avg(community_size) AS avg_size, 
                               max(community_size) AS max_size,
                               min(community_size) AS min_size,
                               count(DISTINCT community) AS total_communities
                    """)

                    record = await result.single()
                    if record and record["avg_size"] is not None:
                        logger.info(f"GraphRAG: ‚úÖ –£—Ä–æ–≤–µ–Ω—å {level} –º–µ—Ç—Ä–∏–∫–∏: "
                                    f"—Å–æ–æ–±—â–µ—Å—Ç–≤={record['total_communities']}, "
                                    f"—Å—Ä.—Ä–∞–∑–º–µ—Ä={record['avg_size']:.1f}, "
                                    f"–º–∞–∫—Å={record['max_size']}, –º–∏–Ω={record['min_size']}")

            except Exception as e:
                logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {e}")

    async def _generate_hierarchical_summaries_safe(self, successful_levels: List[int]):
        """‚úÖ –ú–ê–°–®–¢–ê–ë–ò–†–£–ï–ú–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–æ–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤."""
        async with self.graph_driver.session() as session:
            try:
                for level in successful_levels:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞
                    check_result = await session.run(f"""
                        MATCH (e:Entity) 
                        WHERE e.community_{level} IS NOT NULL
                        RETURN count(e) AS count
                        LIMIT 1
                    """)

                    check_record = await check_result.single()
                    if not check_record or check_record["count"] == 0:
                        logger.debug(f"GraphRAG: –°–≤–æ–π—Å—Ç–≤–æ community_{level} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–≤–æ–¥–∫–∏")
                        continue

                    # ‚úÖ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–æ–∫ - —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ–æ–±—â–µ—Å—Ç–≤
                    result = await session.run(f"""
                        MATCH (e:Entity) 
                        WHERE e.community_{level} IS NOT NULL
                        WITH e.community_{level} AS community, collect(e.name)[0..20] AS entity_names
                        WHERE size(entity_names) >= 3 AND size(entity_names) <= 20
                        RETURN community, entity_names
                        ORDER BY size(entity_names) DESC
                        LIMIT 50
                    """)

                    level_summaries = {}
                    community_count = 0

                    async for record in result:
                        community_id = f"{level}_{record['community']}"
                        entity_names = record["entity_names"]

                        try:
                            summary_prompt = f"""
–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –¥–ª—è —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ —É—Ä–æ–≤–Ω—è {level} (gamma={[0.1, 0.3, 0.5, 0.8, 1.0, 1.5][level]}):

–°—É—â–Ω–æ—Å—Ç–∏: {', '.join(entity_names)}

–û–ø–∏—à–∏ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ–±–ª–∞—Å—Ç—å –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö.
"""
                            summary = self.rm.llm.invoke(summary_prompt)
                            level_summaries[community_id] = {
                                "level": level,
                                "summary": summary.strip(),
                                "entity_count": len(entity_names),
                                "entities": entity_names[:10]
                            }
                            community_count += 1

                        except Exception as e:
                            logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–∫–∏ –¥–ª—è {community_id}: {e}")

                    if level_summaries:
                        self.community_summaries[f"level_{level}"] = level_summaries
                        logger.info(f"GraphRAG: ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(level_summaries)} —Å–≤–æ–¥–æ–∫ –¥–ª—è —É—Ä–æ–≤–Ω—è {level}")

            except Exception as e:
                logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–¥–æ–∫: {e}")

    async def _create_simple_communities(self):
        """–ü—Ä–æ—Å—Ç—ã–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ –∫–∞–∫ fallback."""
        async with self.graph_driver.session() as session:
            try:
                await session.run(
                    "MATCH (e:Entity) "
                    "OPTIONAL MATCH (e)-[:RELATED]-(connected:Entity) "
                    "WITH e, collect(DISTINCT connected.name)[0..10] AS connections "
                    "SET e.community = size(connections)"
                )
                logger.info("GraphRAG: ‚úÖ –°–æ–∑–¥–∞–Ω—ã –ø—Ä–æ—Å—Ç—ã–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (fallback)")
                await self._generate_community_summaries()

            except Exception as e:
                logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–æ–æ–±—â–µ—Å—Ç–≤: {e}")

    async def _generate_community_summaries(self):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫–∏ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–æ–æ–±—â–µ—Å—Ç–≤."""
        async with self.graph_driver.session() as session:
            try:
                result = await session.run(
                    "MATCH (e:Entity) WHERE e.community IS NOT NULL "
                    "WITH e.community AS community, collect(e.name)[0..5] AS entity_names "
                    "WHERE size(entity_names) > 1 "
                    "RETURN community, entity_names "
                    "LIMIT 50"
                )

                async for record in result:
                    community_id = record["community"]
                    entity_names = record["entity_names"]

                    try:
                        summary_prompt = f"""
–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –¥–ª—è –≥—Ä—É–ø–ø—ã —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π:
{', '.join(entity_names)}

–û–ø–∏—à–∏ –æ—Å–Ω–æ–≤–Ω—É—é —Ç–µ–º—É –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö.
"""
                        summary = self.rm.llm.invoke(summary_prompt)
                        self.community_summaries[community_id] = summary.strip()

                    except Exception as e:
                        logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–≤–æ–¥–∫–∏ –¥–ª—è —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ {community_id}: {e}")

            except Exception as e:
                logger.warning(f"GraphRAG: –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–≤–æ–¥–æ–∫ —Å–æ–æ–±—â–µ—Å—Ç–≤: {e}")

    async def graph_search(self, query: str, k: int = 10) -> List[Any]:
        """‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ –∑–Ω–∞–Ω–∏–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤–æ–π—Å—Ç–≤ (–í–°–ï —á–∞–Ω–∫–∏)."""
        if not await self.verify_connection():
            return []

        try:
            prompt = self._query_entity_extraction_prompt.format(query=query)
            raw_response = self.rm.llm.invoke(prompt)
            cleaned_response = self._clean_json_response(raw_response)

            if not cleaned_response:
                query_words = [word.strip().lower() for word in query.split() if len(word.strip()) > 2]
                query_entities = query_words[:5]
            else:
                try:
                    query_entities = json.loads(cleaned_response)
                except:
                    query_entities = []

            if not query_entities:
                logger.debug("GraphRAG: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞")
                return []

            async with self.graph_driver.session() as session:
                # ‚úÖ –ë–ï–ó–û–ü–ê–°–ù–´–ô –ø–æ–∏—Å–∫ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–≤–æ–π—Å—Ç–≤
                result = await session.run(
                    """
                    MATCH (e:Entity) 
                    WHERE toLower(e.name) IN [entity IN $entities | toLower(entity)]
                    WITH e

                    // –ò—â–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω—è—Ö –∏–µ—Ä–∞—Ä—Ö–∏–∏ (–µ—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç)
                    OPTIONAL MATCH (related:Entity)
                    WHERE (
                        // –ü—Ä—è–º—ã–µ —Å–≤—è–∑–∏
                        (e)-[:RELATED*1..2]-(related) OR
                        // –°–æ–æ–±—â–µ—Å—Ç–≤–∞ —É—Ä–æ–≤–Ω—è 0 (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
                        (e.community_0 IS NOT NULL AND e.community_0 = related.community_0) OR
                        // –°–æ–æ–±—â–µ—Å—Ç–≤–∞ —É—Ä–æ–≤–Ω—è 1 (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
                        (e.community_1 IS NOT NULL AND e.community_1 = related.community_1) OR
                        // –°–æ–æ–±—â–µ—Å—Ç–≤–∞ —É—Ä–æ–≤–Ω—è 2 (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
                        (e.community_2 IS NOT NULL AND e.community_2 = related.community_2) OR
                        // –ü—Ä–æ—Å—Ç—ã–µ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ (fallback)
                        (e.community IS NOT NULL AND e.community = related.community)
                    )

                    WITH DISTINCT coalesce(related, e) AS found_entity
                    WHERE found_entity.doc_ids IS NOT NULL
                    UNWIND found_entity.doc_ids AS doc_id
                    RETURN DISTINCT doc_id
                    LIMIT $limit
                    """,
                    entities=query_entities,
                    limit=k * 3
                )

                doc_ids = []
                async for record in result:
                    doc_ids.append(record["doc_id"])

            # ‚úÖ –í–ê–ñ–ù–û: –¢–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –í–°–ï–ì–û –Ω–∞–±–æ—Ä–∞
            relevant_chunks = []
            for doc_id in doc_ids[:k]:
                if 0 <= doc_id < len(self.chunks):
                    relevant_chunks.append(self.chunks[doc_id])

            logger.debug(
                f"GraphRAG: –ù–∞–π–¥–µ–Ω–æ {len(relevant_chunks)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π: {query_entities[:3]}...")
            return relevant_chunks

        except Exception as e:
            logger.error(f"GraphRAG: –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –≥—Ä–∞—Ñ–µ: {e}")
            return []

    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Neo4j."""
        if self.graph_driver:
            await self.graph_driver.close()
            logger.info("GraphRAG: Neo4j —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–æ")


# -----------------------------------------------------------------------------
# 11. ‚úÖ –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –û—Å–Ω–æ–≤–Ω–∞—è RAG —Å–∏—Å—Ç–µ–º–∞ —Å ColBERT –∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏
# -----------------------------------------------------------------------------

class EnhancedAsyncRAGSystem:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ColBERT –∏ GraphRAG."""

    def __init__(self, cfg: RAGConfig):
        self.cfg = cfg
        self.rm = ResourceManager(cfg)
        self.cache = ContextAwareCacheManager(cfg.ttl_cache_sec)  # ‚úÖ –ù–æ–≤—ã–π –∫—ç—à
        self.loader = AdvancedDocumentLoader()
        self.faiss = OptimizedFaissStore(cfg, self.rm)
        self.bm25: Optional[AsyncBM25Retriever] = None
        self.graph_rag = GraphRAGSystem(cfg, self.rm)
        self.use_graph = True
        self.llm_manager = None  # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∑–∂–µ

        # ‚úÖ –°—á–µ—Ç—á–∏–∫ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
        self._query_history = []
        self._max_history = 10

        try:
            self.graph_driver = AsyncGraphDatabase.driver(
                cfg.neo4j_uri, auth=(cfg.neo4j_user, cfg.neo4j_password)
            )
        except Exception:
            self.graph_driver = None

        self._init_lock = asyncio.Lock()
        self._initialized = False
        self.metrics = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "total_response_time": 0.0,
            "colbert_usage": 0
        }

    def _add_to_query_history(self, query: str):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è"""
        query_hash = md5_text(query)
        self._query_history.append({
            'hash': query_hash,
            'timestamp': time.time()
        })

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø—Ä–æ—Å—ã
        if len(self._query_history) > self._max_history:
            self._query_history.pop(0)

    def _is_repeated_query(self, query: str, threshold: float = 0.8) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–º—Å—è"""
        current_hash = md5_text(query)
        current_time = time.time()

        for hist_query in self._query_history:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –º–∏–Ω—É—Ç
            if current_time - hist_query['timestamp'] < 300:
                if hist_query['hash'] == current_hash:
                    logger.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è –∑–∞–ø—Ä–æ—Å")
                    return True
        return False

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã."""
        async with self._init_lock:
            if self._initialized:
                return

            try:
                logger.info("–ù–∞—á–∞–ª–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π RAG-—Å–∏—Å—Ç–µ–º—ã...")
                documents = self.loader.load_documents(self.cfg.folder_path)
                if not documents:
                    raise RuntimeError("–ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞")

                chunks = self.loader.chunk_documents(documents, self.cfg, self.rm)
                if not chunks:
                    raise RuntimeError("–ù–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞")

                # ‚úÖ –í–°–ï –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –í–°–ï —á–∞–Ω–∫–∏
                logger.info(f"üìö FAISS, BM25, ColBERT –∏ GraphRAG: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –∏–∑ {len(chunks)} —á–∞–Ω–∫–æ–≤")

                await self.faiss.build(chunks)
                self.bm25 = AsyncBM25Retriever(chunks)

                # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π LLM –º–µ–Ω–µ–¥–∂–µ—Ä
                self.llm_manager = DynamicLLMManager(self.rm.llm)

                # ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å ColBERT
                if self.cfg.use_colbert_reranker and COLBERT_AVAILABLE:
                    test_reranker = self.rm.colbert_reranker
                    if test_reranker.colbert_model is not None:
                        logger.info("‚úÖ ColBERT reranker –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
                    else:
                        logger.warning("‚ùå ColBERT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ")
                        self.cfg.use_colbert_reranker = False

                # ‚úÖ GraphRAG —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –í–°–ï —á–∞–Ω–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º/–∑–∞–≥—Ä—É–∑–∫–æ–π
                if self.use_graph:
                    logger.info(f"üéØ GraphRAG: –û–±—Ä–∞–±–æ—Ç–∫–∞ –í–°–ï–• {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π")
                    graph_success = await self.graph_rag.initialize_graph(chunks)
                    if graph_success:
                        logger.info("‚úÖ GraphRAG —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ –≤—Å–µ—Ö —á–∞–Ω–∫–∞—Ö")
                    else:
                        logger.warning("‚ùå GraphRAG –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ FAISS+BM25+ColBERT")
                        self.use_graph = False

                self._initialized = True
                logger.info("‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
                logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
                logger.info(f"   ‚Ä¢ FAISS + BM25 + ColBERT + GraphRAG: {len(chunks)} —á–∞–Ω–∫–æ–≤")
                logger.info(f"   ‚Ä¢ ColBERT reranker: {'‚úÖ –í–∫–ª—é—á–µ–Ω' if self.cfg.use_colbert_reranker else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω'}")
                logger.info(
                    f"   ‚Ä¢ GraphRAG —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {'‚úÖ –í–∫–ª—é—á–µ–Ω–æ' if self.cfg.save_graph_structure else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω–æ'}")

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π RAG-—Å–∏—Å—Ç–µ–º—ã: {e}")
                raise

    def _clean_and_deduplicate_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∏ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        if not response:
            return response

        # –£–¥–∞–ª—è–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        response = re.sub(r'\n{3,}', '\n\n', response)

        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        lines = response.split('\n')
        cleaned_lines = []
        seen_lines = set()

        for line in lines:
            line = line.strip()
            if line and line not in seen_lines:
                cleaned_lines.append(line)
                seen_lines.add(line)
            elif not line:  # –ü—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º
                cleaned_lines.append(line)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
        cleaned_response = '\n'.join(cleaned_lines)

        # –£–¥–∞–ª—è–µ–º —Ñ—Ä–∞–∑—ã-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
        repetition_patterns = [
            r'(–ö–∞–∫ —è —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª,?\s*)',
            r'(–ü–æ–≤—Ç–æ—Ä—è—é,?\s*)',
            r'(–ï—â–µ —Ä–∞–∑,?\s*)',
            r'(–°–Ω–æ–≤–∞,?\s*)',
            r'(As I mentioned,?\s*)',
            r'(Again,?\s*)',
        ]

        for pattern in repetition_patterns:
            cleaned_response = re.sub(pattern, '', cleaned_response, flags=re.IGNORECASE)

        return cleaned_response.strip()

    async def process_query(self, query: str, k: int = 5, include_time: bool = True, user_id: str = None) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å —É—á–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        start_time = time.time()
        self.metrics["total_queries"] += 1

        try:
            await self.initialize()

            # ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–∞–ø—Ä–æ—Å—ã
            if self._is_repeated_query(query):
                logger.warning(f"–ü–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è –∑–∞–ø—Ä–æ—Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω: {query[:50]}...")
                return "–≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —É–∂–µ –±—ã–ª –Ω–µ–¥–∞–≤–Ω–æ –∑–∞–¥–∞–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –ø–æ-–¥—Ä—É–≥–æ–º—É."

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self._add_to_query_history(query)

            # ‚úÖ –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Å —É—á–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            cached = self.cache.get(query, user_id)
            if cached:
                logger.debug(f"Cache hit –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {query[:50]}...")
                response_time = time.time() - start_time
                if include_time:
                    return f"{cached}\n‚è± –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response_time:.2f} —Å–µ–∫ (–∏–∑ –∫—ç—à–∞)"
                return cached

            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {query[:100]}...")

            search_tasks = [
                self.faiss.search(query, k=15),
                self.bm25.retrieve(query, k=15)
            ]

            if self.use_graph:
                search_tasks.append(self.graph_rag.graph_search(query, k))

            results = await asyncio.gather(*search_tasks, return_exceptions=True)

            faiss_results = results[0] if not isinstance(results[0], Exception) else []
            bm25_results = results[1] if not isinstance(results[1], Exception) else []
            graph_results = results[2] if self.use_graph and len(results) > 2 and not isinstance(results[2],
                                                                                                 Exception) else []

            # ‚úÖ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            logger.info(
                f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: FAISS={len(faiss_results)}, BM25={len(bm25_results)}, Graph={len(graph_results)}")

            if not faiss_results and not bm25_results and not graph_results:
                logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∏ –æ–¥–Ω–∏–º –º–µ—Ç–æ–¥–æ–º –ø–æ–∏—Å–∫–∞")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."

            all_result_lists = [faiss_results, bm25_results]
            if graph_results:
                all_result_lists.append(graph_results)

            fused = self._rrf_fusion(all_result_lists, k_rrf=60)
            unique_docs = self._remove_duplicates(fused)

            if len(unique_docs) > k:
                # ‚úÖ –ò–°–ü–û–õ–¨–ó–£–ï–ú COLBERT –î–õ–Ø –ü–ï–†–ï–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–Ø
                if self.cfg.use_colbert_reranker:
                    reranked = self.rm.colbert_reranker.rerank(query, unique_docs[:15], k=k)
                    self.metrics["colbert_usage"] += 1
                    logger.debug(f"ColBERT –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: {len(unique_docs)} -> {len(reranked)}")
                else:
                    reranked = unique_docs[:k]
            else:
                reranked = unique_docs

            if not reranked:
                logger.warning("–ü–æ—Å–ª–µ –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∫–∏ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                return "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞."

            # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π LLM –º–µ–Ω–µ–¥–∂–µ—Ä
            answer = self._generate_answer_with_diversity(query, reranked, user_id)

            # ‚úÖ –û–ß–ò–°–¢–ö–ê –û–¢–í–ï–¢–ê –û–¢ –ü–û–í–¢–û–†–ï–ù–ò–ô
            cleaned_answer = self._clean_and_deduplicate_response(answer)

            # ‚úÖ –ö—ç—à–∏—Ä—É–µ–º —Å —É—á–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            self.cache.set(query, cleaned_answer, user_id)

            response_time = time.time() - start_time
            self.metrics["successful_queries"] += 1
            self.metrics["total_response_time"] += response_time

            logger.info(f"–ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {response_time:.2f} —Å–µ–∫ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

            if include_time:
                return f"{cleaned_answer}\n‚è± –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response_time:.2f} —Å–µ–∫"
            return cleaned_answer

        except Exception as e:
            self.metrics["failed_queries"] += 1
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"

    def _generate_answer_with_diversity(self, query: str, docs: List[Any], user_id: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        try:
            contexts = []
            for doc in docs:
                if hasattr(doc, 'page_content'):
                    contexts.append(doc.page_content)
                else:
                    contexts.append(str(doc))

            context = "\n---\n".join(contexts)

            # ‚úÖ –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–†–û–ú–¢ –î–õ–Ø –ü–†–ï–î–û–¢–í–†–ê–©–ï–ù–ò–Ø –ü–û–í–¢–û–†–ï–ù–ò–ô
            prompt = f"""–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ—á–Ω—ã–µ, —è—Å–Ω—ã–µ –∏ –∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
        - –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        - –ù–ï –ø–æ–≤—Ç–æ—Ä—è–π –æ–¥–Ω—É –∏ —Ç—É –∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
        - –ò–∑–±–µ–≥–∞–π —Ñ—Ä–∞–∑ —Ç–∏–ø–∞ "–∫–∞–∫ —è —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª", "–ø–æ–≤—Ç–æ—Ä—è—é", "–µ—â–µ —Ä–∞–∑"
        - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ª–æ–≥–∏—á–Ω–æ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
        - –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–∑–±–µ–≥–∞–π –æ–±—â–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π

        –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
        1. –î–∞–π –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
        2. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        3. –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º —á–µ—Å—Ç–Ω–æ
        4. –†–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞–π —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–∏
        5. –ó–∞–∫–∞–Ω—á–∏–≤–∞–π –æ—Ç–≤–µ—Ç, –∫–æ–≥–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏—Å—á–µ—Ä–ø–∞–Ω–∞

        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context}

        –í–æ–ø—Ä–æ—Å: {query}

        –û—Ç–≤–µ—Ç:"""

            # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π LLM –º–µ–Ω–µ–¥–∂–µ—Ä
            answer = self.llm_manager.generate_response(prompt, user_id)
            self.rm.gpu_clear()

            return answer

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

    def clear_user_context(self, user_id: str):
        """–û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if hasattr(self.cache, 'clear_user_context'):
            self.cache.clear_user_context(user_id)
        if hasattr(self.llm_manager, 'clear_user_history'):
            self.llm_manager.clear_user_history(user_id)
        logger.info(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –æ—á–∏—â–µ–Ω")

    def _rrf_fusion(self, result_lists: List[List[Any]], k_rrf: int = 60) -> List[Any]:
        """Reciprocal Rank Fusion –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞."""
        doc_scores = {}
        for result_list in result_lists:
            for rank, doc in enumerate(result_list):
                doc_hash = md5_text(doc.page_content if hasattr(doc, 'page_content') else str(doc))
                doc_scores[doc_hash] = doc_scores.get(doc_hash, 0.0) + 1.0 / (rank + 1 + k_rrf)

        docs_dict = {}
        for result_list in result_lists:
            for doc in result_list:
                doc_hash = md5_text(doc.page_content if hasattr(doc, 'page_content') else str(doc))
                docs_dict.setdefault(doc_hash, doc)

        sorted_docs = []
        for doc_hash, score in sorted(doc_scores.items(), key=lambda x: x[1], reverse=True):
            if doc_hash in docs_dict:
                sorted_docs.append(docs_dict[doc_hash])

        return sorted_docs

    def _remove_duplicates(self, docs: List[Any]) -> List[Any]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É."""
        seen = set()
        unique = []

        for doc in docs:
            content_hash = md5_text(doc.page_content if hasattr(doc, 'page_content') else str(doc))
            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(doc)

        logger.debug(f"–£–¥–∞–ª–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã: {len(docs)} -> {len(unique)}")
        return unique

    def get_metrics(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã."""
        total_queries = max(self.metrics["total_queries"], 1)
        success_rate = (self.metrics["successful_queries"] / total_queries) * 100
        avg_response_time = self.metrics["total_response_time"] / max(self.metrics["successful_queries"], 1)

        return {
            **self.metrics,
            "success_rate": f"{success_rate:.2f}%",
            "average_response_time": f"{avg_response_time:.2f}s",
            "graph_enabled": self.use_graph,
            "colbert_enabled": self.cfg.use_colbert_reranker,
            **self.cache.get_stats()
        }

    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∏ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã."""
        try:
            logger.info("üîÑ –ó–∞–∫—Ä—ã—Ç–∏–µ RAG-—Å–∏—Å—Ç–µ–º—ã...")

            if self.use_graph and hasattr(self, 'graph_rag'):
                await self.graph_rag.close()

            if hasattr(self, 'graph_driver') and self.graph_driver:
                await self.graph_driver.close()

            self.rm.gpu_clear()

            logger.info("‚úÖ RAG-—Å–∏—Å—Ç–µ–º–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–∫—Ä—ã—Ç–∞")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ RAG-—Å–∏—Å—Ç–µ–º—ã: {e}")

    # -----------------------------------------------------------------------------
    # 12. –ì–ª–∞–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è RAG-—Å–∏—Å—Ç–µ–º–∞ (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
    # -----------------------------------------------------------------------------

class AsyncRAGSystem(EnhancedAsyncRAGSystem):
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∫–ª–∞—Å—Å–æ–º."""
    pass

    # -----------------------------------------------------------------------------
    # 13. ‚úÖ –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å ColBERT –∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏
    # -----------------------------------------------------------------------------

async def main():
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π RAG-—Å–∏—Å—Ç–µ–º–æ–π."""
        cfg = RAGConfig(
            folder_path=os.getenv("RAG_DOCS_PATH", "docs/"),
            faiss_path=os.getenv("RAG_FAISS_PATH", "vector_store/faiss_index"),
            neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
            neo4j_password=os.getenv("NEO4J_PASSWORD", "neo4jneo4j"),
            use_semantic_chunking=True,
            # ‚úÖ –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –ü–ï–†–í–û–ì–û/–ü–û–í–¢–û–†–ù–û–ì–û –ó–ê–ü–£–°–ö–ê
            force_rebuild_graph=True,  # –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ True, –ø–æ—Ç–æ–º False
            save_graph_structure=True,  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –≥—Ä–∞—Ñ
            auto_load_existing=True,  # –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö
            graph_metadata_dir="graph_data/",  # –ü–∞–ø–∫–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            # ‚úÖ –í–ö–õ–Æ–ß–ê–ï–ú COLBERT
            use_colbert_reranker=True,
            colbert_model="colbert-ir/colbertv2.0",
            # ‚úÖ –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò LLM
            llm_temperature=0.4,
            llm_repeat_penalty=1.05,
            llm_repeat_last_n=128,
            llm_top_p=0.85,
            llm_top_k=40,
        )

        rag = EnhancedAsyncRAGSystem(cfg)

        print("\nüöÄ –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é RAG-—Å–∏—Å—Ç–µ–º—É —Å ColBERT, GraphRAG –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã!")
        print("üìÅ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: PDF, DOCX, TXT")
        print("‚ö° –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
        print("   ‚Ä¢ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ FAISS —Å Qwen3-Embedding-0.6B")
        print("   ‚Ä¢ BM25 –ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫")
        print("   ‚Ä¢ ColBERT –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏")
        print("   ‚Ä¢ GraphRAG —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ–º –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π")
        print("   ‚Ä¢ –ê–ª–≥–æ—Ä–∏—Ç–º –õ–µ–π–¥–µ–Ω–∞ –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —Å–æ–æ–±—â–µ—Å—Ç–≤ (–∫–∞–∫ –≤ Microsoft GraphRAG)")
        print("   ‚Ä¢ üÜï –°–û–•–†–ê–ù–ï–ù–ò–ï –∏ –ó–ê–ì–†–£–ó–ö–ê —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥—Ä–∞—Ñ–∞ –∏ —Å–æ–æ–±—â–µ—Å—Ç–≤ –õ–µ–π–¥–µ–Ω–∞")
        print("   ‚Ä¢ –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
        print("   ‚Ä¢ –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
        print("‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
        print("   ‚Ä¢ Neo4j GDS 2.20 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (gamma –≤–º–µ—Å—Ç–æ resolution)")
        print("   ‚Ä¢ –ù–µ–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –¥–ª—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –õ–µ–π–¥–µ–Ω–∞")
        print("   ‚Ä¢ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM –ø—Ä–æ—Ç–∏–≤ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π")
        print("   ‚Ä¢ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞:")
        print(f"   ‚Ä¢ –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ: –≥—Ä–∞—Ñ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        print(f"   ‚Ä¢ –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö: –≥—Ä–∞—Ñ –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ ~30-60 —Å–µ–∫—É–Ω–¥")
        print(f"   ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤: {cfg.graph_metadata_dir}")
        print("\nüìö –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ü–û–õ–ù–û–ô –∫–æ–ª–ª–µ–∫—Ü–∏–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print("–ö–æ–º–∞–Ω–¥—ã:")
        print("  ‚Ä¢ –í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞")
        print("  ‚Ä¢ 'clear' - –æ—á–∏—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
        print("  ‚Ä¢ 'metrics' - –ø–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã")
        print("  ‚Ä¢ 'exit' - –≤—ã—Ö–æ–¥\n")

        try:
            user_id = "default_user"  # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –±–µ—Ä–µ—Ç—Å—è –∏–∑ Telegram

            while True:
                query = input("üîç –í–∞—à –∑–∞–ø—Ä–æ—Å: ").strip()

                if not query:
                    continue

                if query.lower() == 'exit':
                    break

                if query.lower() == 'clear':
                    rag.clear_user_context(user_id)
                    print("üßπ –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ—á–∏—â–µ–Ω!\n")
                    continue

                if query.lower() == 'metrics':
                    metrics = rag.get_metrics()
                    print("\nüìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã:")
                    print("=" * 50)
                    for key, value in metrics.items():
                        if key == "colbert_usage":
                            print(f"  üìà ColBERT –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π: {value}")
                        elif key == "graph_enabled":
                            print(f"  üï∏Ô∏è GraphRAG: {'‚úÖ –í–∫–ª—é—á–µ–Ω' if value else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω'}")
                        elif key == "colbert_enabled":
                            print(f"  üéØ ColBERT: {'‚úÖ –í–∫–ª—é—á–µ–Ω' if value else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω'}")
                        else:
                            print(f"  {key}: {value}")
                    print("=" * 50)
                    print()
                    continue

                try:
                    print("‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å...")
                    answer = await rag.process_query(query, user_id=user_id, include_time=True)
                    print(f"\nüí° {answer}\n")
                    print("-" * 80)
                except Exception as e:
                    print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}\n")

        except KeyboardInterrupt:
            print("\n\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        finally:
            await rag.close()
            print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    # -----------------------------------------------------------------------------
    # 14. ‚úÖ –¢–û–ß–ö–ê –í–•–û–î–ê
    # -----------------------------------------------------------------------------

if __name__ == "__main__":
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")

        required_packages = {
            "faiss": "faiss-cpu",
            "torch": "torch",
            "langchain_community": "langchain-community",
            "langchain_huggingface": "langchain-huggingface",
            "langchain_ollama": "langchain-ollama",
            "neo4j": "neo4j",
            "cachetools": "cachetools",
        }

        missing_packages = []
        for package, install_name in required_packages.items():
            try:
                __import__(package)
                print(f"‚úÖ {package}")
            except ImportError:
                print(f"‚ùå {package} (—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install {install_name})")
                missing_packages.append(install_name)

        if not COLBERT_AVAILABLE:
            print("‚ùå ragatouille (—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ragatouille)")
            missing_packages.append("ragatouille")
        else:
            print("‚úÖ ragatouille (ColBERT)")

        if missing_packages:
            print(f"\nüö® –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–∞–∫–µ—Ç—ã:")
            print(f"pip install {' '.join(missing_packages)}")
            print("\n–¢–∞–∫–∂–µ —É–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:")
            print("‚Ä¢ Neo4j –∑–∞–ø—É—â–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω")
            print("‚Ä¢ Ollama –∑–∞–ø—É—â–µ–Ω —Å –º–æ–¥–µ–ª—å—é qwen2.5vl:7b")
            print("‚Ä¢ –ü–∞–ø–∫–∞ 'docs/' —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞—à–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã")
            exit(1)

        print("‚úÖ –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
        print("\nüöÄ –ó–∞–ø—É—Å–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π RAG-—Å–∏—Å—Ç–µ–º—ã...\n")

        # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
        asyncio.run(main())

